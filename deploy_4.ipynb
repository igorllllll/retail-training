{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "148f3409-4040-40b2-8e28-7413b6cd02c9",
   "metadata": {},
   "source": [
    "#### Converter o modelo para o formato ONNX e usar uma pipeline otimizada\n",
    "\n",
    "Essa etapa tem como objetivo converter o modelo para o formato ONNX, que é usado para exportar modelos treinados para aplicações em produção. Esse formato é leve, otimizado e facilita a integração com diferentes frameworks e ambientes. No entanto, devido às limitações de recursos computacionais mencionadas na etapa de treinamento, não consegui realizar a conversão neste caso. Mesmo assim, decidi incluir o conceito no projeto para mostrar sua relevância e destacar como ele pode ser explorado em cenários futuros.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "271ea048-0f7a-403b-8493-16a19159071e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import glob\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from super_gradients.training import models\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "CLASSES = ['product']\n",
    "CLASSES += [str(i) for i in range(80 - len(CLASSES))]\n",
    "\n",
    "model = models.get(\n",
    "    \"yolo_nas_s\",\n",
    "    num_classes=len(CLASSES),\n",
    "    checkpoint_path=f\"./weights/SKU110K/RUN_20241215_173503_278811/average_model.pth\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ed63639-f0d2-4091-af6c-e5b303a4e760",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./weights/SKU110K/RUN_20241215_173503_278811/average_model.onnx'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models.convert_to_onnx(model=model, input_shape=(3,640,640), out_path=\"./weights/SKU110K/RUN_20241215_173503_278811/average_model.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36126b9e-5bb2-432f-8c90-a3b3bfadb99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yolo_nas_onnx.models import load_net\n",
    "from yolo_nas_onnx.processing import Preprocessing, Postprocessing\n",
    "from yolo_nas_onnx.draw import draw_box\n",
    "from yolo_nas_onnx.utils import Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92daba84-a67e-4974-8cef-e01806f83e27",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "D:\\a\\_work\\1\\s\\onnxruntime\\python\\onnxruntime_pybind_state.cc:562 onnxruntime::python::CreateExecutionProviderInstance CUDA_PATH is set but CUDA wasn't able to be loaded. Please install the correct version of CUDA and cuDNN as mentioned in the GPU requirements page (https://onnxruntime.ai/docs/reference/execution-providers/CUDA-ExecutionProvider.html#requirements), make sure they're in the PATH, and that your GPU is supported.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m use_opencv_dnn_runtime \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     21\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./weights/SKU110K/RUN_20241215_173503_278811/average_model.onnx\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 23\u001b[0m net \u001b[38;5;241m=\u001b[39m \u001b[43mload_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_gpu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_opencv_dnn_runtime\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m net\u001b[38;5;241m.\u001b[39massert_input_shape([\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m640\u001b[39m,\u001b[38;5;241m640\u001b[39m])\n\u001b[0;32m     25\u001b[0m net\u001b[38;5;241m.\u001b[39mwarmup()\n",
      "File \u001b[1;32m~\\Desktop\\Projects\\Jupyter Projects\\retail_product_test_1\\yolo_nas_onnx\\models.py:113\u001b[0m, in \u001b[0;36mload_net\u001b[1;34m(path, gpu, dnn)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dnn:  \u001b[38;5;66;03m# if using OpenCV DNN\u001b[39;00m\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DNN_LOADER(path, gpu)\n\u001b[1;32m--> 113\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mORT_LOADER\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgpu\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\Projects\\Jupyter Projects\\retail_product_test_1\\yolo_nas_onnx\\models.py:12\u001b[0m, in \u001b[0;36mORT_LOADER.__init__\u001b[1;34m(self, path, gpu)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, path: \u001b[38;5;28mstr\u001b[39m, gpu: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m---> 12\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgpu\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\Projects\\Jupyter Projects\\retail_product_test_1\\yolo_nas_onnx\\models.py:26\u001b[0m, in \u001b[0;36mORT_LOADER._load_model\u001b[1;34m(self, path, use_gpu)\u001b[0m\n\u001b[0;32m     19\u001b[0m     use_gpu \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     21\u001b[0m providers \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     22\u001b[0m     [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDAExecutionProvider\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCPUExecutionProvider\u001b[39m\u001b[38;5;124m\"\u001b[39m]  \u001b[38;5;66;03m# use cuda if gpu is available\u001b[39;00m\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_gpu\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCPUExecutionProvider\u001b[39m\u001b[38;5;124m\"\u001b[39m]  \u001b[38;5;66;03m# use CPU\u001b[39;00m\n\u001b[0;32m     25\u001b[0m )  \u001b[38;5;66;03m# get providers\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet \u001b[38;5;241m=\u001b[39m \u001b[43mort\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mInferenceSession\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproviders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproviders\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# load session\u001b[39;00m\n\u001b[0;32m     28\u001b[0m net_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet\u001b[38;5;241m.\u001b[39mget_inputs()[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# get input info\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_name \u001b[38;5;241m=\u001b[39m net_input\u001b[38;5;241m.\u001b[39mname\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\onnxruntime\\capi\\onnxruntime_inference_collection.py:347\u001b[0m, in \u001b[0;36mInferenceSession.__init__\u001b[1;34m(self, path_or_bytes, sess_options, providers, provider_options, **kwargs)\u001b[0m\n\u001b[0;32m    344\u001b[0m disabled_optimizers \u001b[38;5;241m=\u001b[39m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisabled_optimizers\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisabled_optimizers\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 347\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_inference_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproviders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprovider_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisabled_optimizers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[0;32m    349\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_fallback:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\onnxruntime\\capi\\onnxruntime_inference_collection.py:395\u001b[0m, in \u001b[0;36mInferenceSession._create_inference_session\u001b[1;34m(self, providers, provider_options, disabled_optimizers)\u001b[0m\n\u001b[0;32m    392\u001b[0m     disabled_optimizers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(disabled_optimizers)\n\u001b[0;32m    394\u001b[0m \u001b[38;5;66;03m# initialize the C++ InferenceSession\u001b[39;00m\n\u001b[1;32m--> 395\u001b[0m \u001b[43msess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitialize_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproviders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprovider_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisabled_optimizers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    397\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sess \u001b[38;5;241m=\u001b[39m sess\n\u001b[0;32m    398\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sess_options \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sess\u001b[38;5;241m.\u001b[39msession_options\n",
      "\u001b[1;31mRuntimeError\u001b[0m: D:\\a\\_work\\1\\s\\onnxruntime\\python\\onnxruntime_pybind_state.cc:562 onnxruntime::python::CreateExecutionProviderInstance CUDA_PATH is set but CUDA wasn't able to be loaded. Please install the correct version of CUDA and cuDNN as mentioned in the GPU requirements page (https://onnxruntime.ai/docs/reference/execution-providers/CUDA-ExecutionProvider.html#requirements), make sure they're in the PATH, and that your GPU is supported.\n"
     ]
    }
   ],
   "source": [
    "def detect(net, source, pre_process, post_process, labels):\n",
    "    net_input = source.copy()  \n",
    "    input_, prep_meta = pre_process(net_input)  \n",
    "    outputs = net.forward(input_) \n",
    "\n",
    "    boxes, scores, classes = post_process(outputs, prep_meta)  \n",
    "    selected = cv2.dnn.NMSBoxes(\n",
    "        boxes, scores, post_process.score_thres, post_process.iou_thres\n",
    "    ) \n",
    "\n",
    "    for i in selected:  \n",
    "        box = boxes[i, :].astype(np.int32).flatten()  \n",
    "        score = float(scores[i]) * 100  \n",
    "        label, color = labels(classes[i], use_bgr=True) \n",
    "\n",
    "        draw_box(source, box, label, score, color) \n",
    "    return source \n",
    "\n",
    "use_gpu = True\n",
    "use_opencv_dnn_runtime = False\n",
    "model_path = \"./weights/SKU110K/RUN_20241215_173503_278811/average_model.onnx\"\n",
    "\n",
    "net = load_net(model_path, use_gpu, use_opencv_dnn_runtime)\n",
    "net.assert_input_shape([1,3,640,640])\n",
    "net.warmup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df2808a-7776-444c-a0a0-19299719c601",
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_steps = [\n",
    "    {\"DetLongMaxRescale\": None},\n",
    "    {\"BotRightPad\": {\"pad_value\": 114}}\n",
    "]\n",
    "\n",
    "iou_thres = 0.65\n",
    "score_thres = 0.5\n",
    "labels = [\"0\"]\n",
    "\n",
    "_, _, input_height, input_width = net.input_shape  # get input height and width [b, c, h, w]\n",
    "\n",
    "pre_process = Preprocessing(\n",
    "    prep_steps, (input_height, input_width)\n",
    ")\n",
    "\n",
    "post_process = Postprocessing(\n",
    "    prep_steps,\n",
    "    iou_thres,\n",
    "    score_thres,\n",
    ")\n",
    "\n",
    "labels = Labels(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33aadc5d-aee2-46d0-bc87-89454ae71667",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "img = cv2.imread(\"./SKU110K_fixed/images/test/test_0.jpg\")\n",
    "img = detect(net, img, pre_process, post_process, labels)\n",
    "\n",
    "Image.fromarray(img[:,:,::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83e5e3a-cd5a-4840-a5f6-da4d7790da3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
